%        File: summary_revised.tex
%     Created: Mon Feb 16 05:00 AM 2015 C
% Last Change: Mon Feb 16 05:00 AM 2015 C
%
\documentclass[a4paper]{report}

% Mathematical typesetting
\usepackage{amsmath}
\usepackage{amsthm}

% Maths related command definitions
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{procedure}{Procedure}
\theoremstyle{plain}
\newtheorem{prop}{Proposition}
\newtheorem{axiom}{Axioms}


\begin{document}

\tableofcontents

\chapter{Overview and Descriptive Statistics}
\section{Populations and Samples}
\section{Pictorial and Tabular Methods in Descriptive Statistics}

\begin{procedure}[Steps for constructing a stem-and-leaf display]
\begin{enumerate} 
  \item Select one or more leading digits for the stem values.
        The trailing digits becomes the leaves.
  \item List possible stem values in a vertical column.
  \item Record the leaf for every observation beside the corresponding stem value.
  \item Order the leaves from smallest to largest on each line.
  \item Indicate the units for stems and ;eaves someplace in the display.
\end{enumerate}
\end{procedure}

\begin{procedure}[A histogram for counting data]
  First, determine the frequency and relative frequency of each $x$ value. Then
  mark possible $x$ values on a horizontal scale. Above each calue, draw a
  rectangle whose height is the relative frequency (or alternatively, the
  frequency of that value).
\end{procedure}

\begin{procedure}[A histogram for measurement data: equal class widths]
  Determine the frequency and relative frequency for each class. Mark the class
  boundaries on a horizontal measurement axis.  Above each class interval, draw
  a rectangle whose height is the corresponding relative frequence (or
  frequency).
\end{procedure}

\begin{procedure}[A histogram for measurement data: unequal class widths]
  After determining frequencies and relative frequencies, calculate the height
  of each rectangle using the formula

  \begin{equation*}
    \text{rectangle height} = \frac{\text{relative frequency of the class}}{\text{class width}}
  \end{equation*}

  The resulting rectangle heights are usually called \textit{densities}, and
  the vertical scale is the \textbf{density scale}. This prescription will also
  work when class widths are equal.
\end{procedure}

\section{Measures of Location}

\begin{definition}
  The \textbf{sample mean} $\bar{x}$ of observations $x_1, x_2, \dots, x_n$ is given by
  \begin{equation}
    \bar{x} = \frac{x_1 + x_2 + \hdots + x_n}{n} = \frac{\sum\limits_{i=1}^n x_i}{n}.
  \end{equation}
  The numerator of $\bar{x}$ can be written more informally as $\sum\nolimits
  x_i$ where the summation is over all sample observations.
\end{definition}

\begin{definition}
  The \textbf{sample mean} is obtained by first ordering the $n$ observations
  from smallest to largest (with any repeated values included so that every
  sample observation appears in the ordered list). Then, 
  
  \[
    \bar{x} = \begin{cases}
      {} & \text{The single middle value if $n$ is odd} \\
      {} & \text{The average of the two middle values if $n$ is even}
    \end{cases}
  \]
\end{definition}

\section{Measures of Variability}

\begin{definition}
  The \textbf{sample variance}, denoted by $s^2$, is given by
  \begin{equation*}
    s^2 = \frac{\left(\sum\nolimits(x_i - \bar{x}\right)^2}{n-1} = \frac{S_{xx}}{n_1}
  \end{equation*}
  The \textbf{sample standard deviation}, denoted by $s$, is the (positive)
  square root of the variance:
  \begin{equation*}
    s = \sqrt{s^2}
  \end{equation*}
\end{definition}

\begin{prop}
  Let $x_1, x_2, \dots, x_n$ be a sample and $c$ be a constant.

  \begin{enumerate}
    \item If $y_1 = x_1 + c, y_2 = x_2 + c, \dots, y_n = x_n + c, $ then $s_y^2 = s_x^2$, and
    \item If $y_1 = cx_1, \dots, y_n = cx_n, $ then $s_y^2 = c^2s_x^2, sy = |c|s_x, $
  \end{enumerate}
  where $s_x^2$ is the sample variance of the $x$'s and $s_y^2$ is the sample
  variance of the $y$'s.
\end{prop}

\begin{definition}
  Order the $n$ observations from smallest to largest and separate the samllest
  half from the largest half; the median $\bar{x}$ is included in both halves
  if $n$ is odd. Then the \textbf{lower fourth} is the median of the smallest
  half and the \textbf{upper fourth} is the median of the largest half. A
  measure of spread that is resistant to outliers is the \textbf{fourth spread}
  $f_s$, given by
  \begin{equation*}
    f_s = \text{upper fourth - lower fourth}
  \end{equation*}
\end{definition}

\begin{definition}
  Any observation farthen than $1.5f_s$ from the closts fourth is an \textbf{outlier}. An outlier is \textbf{extreme} if it is more than $3f_s$ from the nearest
  fourth, and it is \textbf{mild} otherwise.
\end{definition}
\chapter{Probability}

\section{Sample Spaces and Events}
\begin{definition} 
  The \textbf{sample space} of an experiment, denoted by $\mathcal{S}$,
  is the set of all possible outcomes of that experiment.
\end{definition}

\begin{definition}
  An \textbf{event} is any collection (subset) of outcomes contained in
  the sample space $\mathcal{S}$. An event is said to be \textbf{simple}
  if it consists of exactly one outcome and \textbf{compound} if it
  consists of more than one outcome.
\end{definition}

\begin{definition}
  Some relations from set theory:
  \begin{enumerate}
    \item The \textbf{union} of two events $A$ and $B$, denoted by $A\cup B$ and
      read ``$A$ or $B$'' is the event consisting of all outcomes that are
      \textit{either in $A$ or in $B$ or in both events} (so that the union
      includes outcomes for which both $A$ and $B$ occur as well as the outcomes
      for which exactly one occurs) - that is, all outcomes in at least one of
      the events.
    \item The \textbf{intersection} of two events $A$ and $B$, denoted by $A \cap
      B$ and read `` \textit{$A$ and $B$} '' is the event consisting of all
      outcomes that are in \textit{both $A$ and $B$}.
    \item The \textbf{complement} of an event $A$, denoted by $A'$, is the set of
      all outcomes in $\mathcal{S}$ that are not contained in $A$.
  \end{enumerate}
\end{definition}

\begin{definition}
  When $A$ and $B$ have no outcomes in common, they are said to be
  \textbf{disjoint} or \textbf{mutually exclusive} events.  Mathematicians write
  this compactly as $A \cup B = \emptyset$ where $\emptyset$ denotes the event
  consisting of no outcomes whatsoever (the ``null'' or ``empty'' event).
\end{definition}

\section{Axioms, Interpretations, and Properties of Probability}
\begin{definition}
  Basic properties of probability:
  \begin{enumerate}
    \item For any event $A, P(A) \geq 0$.
    \item $P(S) = 1$.
    \item If $A_1, A_2, A_3, \cdots $ is an infinite collection of disjoint events, then \[P(A_1 \cup A_2 \cup \cdots) = \sum\limits_{i=1}^\infty P(A_i).\]
  \end{enumerate}
\end{definition}

\begin{prop}
$P(\emptyset) = 0$ where $\emptyset$ is the null event. This in turn
  implies that the property contained in Axiom 3 is valid for a \textit{finite}
  collection of events.\footnote{Proof page 57.}
\end{prop}

\begin{prop}
For any event $A, P(A) = 1 - P(A')$.\footnote{Proof page 60.}
\end{prop}

\begin{prop}
  For any event $A, P(A) \leq 1$.
\end{prop}

\begin{prop}
  For any events $A$ and $B$,
  \begin{align*}
    P(A\cup B) &= P(A) + P(B) - P(A\cap B).\footnote{Proof page 61}\\
    P(A \cup B \cup C) &= P(A) + P(B) + P(C) - P(A \cap B) - P(A \cap C)  \\
                       &\quad {}- P(A \cap C) - P(B \cap C) + P(A \cap B \cap C)
  \end{align*}
\end{prop}

\section{Counting Techniques}
\begin{prop}
  If the first element or object of an ordered pair can be selected in $n_1$
  ways, and for each of these $n_1$ ways the second element of the pair can be
  selected in $n_2$ ways, then the number of pairs is $n_1n_2$.
\end{prop}

\begin{prop}[Product rule for $K$-tuples]
  Suppose a set consists of ordered collections of $k$ elements ($k$-tuples)
  and that there are $n_1$ possible choices for the first element; for each
  choice of the first element, there are $n_2$ possible choices of the second
  element;\dots; for each possible of the first $k-1$ elements, there are $n_k$
  choices for $k$th element. Then there are $n_1n_2\cdot\dots\cdot n_k$
  possible $k$-tuples.  
\end{prop}

\begin{definition}
  Any ordered sequence $k$ objects taken from a set of $n$ distinct objects is
  called a \textbf{permutation} of size $k$ of the objects.  The numbers of
  permutations of size $k$ that can be constructed from the $n$ objects is
  denoted $P_{k, n}$.
\end{definition}

\begin{definition} 
  For any positive integer $m$, $m!$ is read ``$m$ factorial'' and is defined
  by $m! = m(m-1)(m-2)\dots(2)(1)$. Also, $0! = 1$.
\end{definition}

\begin{definition}
  Given a set of $n$ distinct objects, any unordered subset of size $k$ of the
  objects is colled a \textbf{combination}. The number of combinations of size
  $k$ that can be formed from $n$ will be denoted by ${n \choose k}$. This
  notation is more common in probability
  than $C_{k,n}$, which would be analogous to notation for permutations.
\end{definition}
\section{Conditional Probability}

\begin{definition} 
  For any two events $A$ and $B$ with $P(B) > 0$ , the
  \textbf{conditional probability of $A$ given $B$ that has occured} is defined
  by \begin{align*} P(A\mid B) = \frac{P(A\cap B)}{P(B)}.  \end{align*}
\end{definition}

\begin{prop}[The multiplication rule]
  $P(A \cap B) = P(A \mid B) \cdot P(B)$
\end{prop}

\begin{prop}[The law of total probability]
  Let $A_1, \dots, A_k$ be mutually exclusive and exhaustive events. Then for any other event $B$.
  \begin{align*}
    P(B)&= P(B \mid A_1) \cdot P(A_1) + \dots + P(B \mid A_k) \cdot P(A_k)\\
        &= \sum\limits_{i=1}^{k} P(B \mid A_i)P(A_i)
  \end{align*}
\end{prop}

\begin{prop}[Bayes' Theorem]
  Let $A_1, \dots, A_k$ be a collection of mutually exclusive and exhaustive
  events with $P(A_i) > 0$ for $i = 0,\dots,k$. Then for any other event $B$,
  for which $P(B) > 0$
  \begin{align*}
    P(A_j \mid B) = \frac{P(A_j \cap B)}{P(B)} = \frac{P(B \mid A_j)P(A_j)}{\sum\limits_{i=1}^k P(B \mid A_i)P(A_i)} \qquad j = 0,\dots,k
  \end{align*}
\end{prop}

\section{Independence}
\begin{definition}
  Two events $A$ and $B$ are \textbf{independent} if $P(A \mid B) = P(A)$ and
  are \textbf{dependend} otherwise.
\end{definition}

\begin{prop}
  $A$ and $B$ are independent if and only if
  \begin{align*}
    P(A \cap B) = P(A) \cdot P(B)  
  \end{align*}
\end{prop}

\begin{definition}
  Events $A_1, \dots, A_n$ are \textbf{mutually independent} if for
  every $k (k = 2, 3, \dots, n)$ and every subset of indices $i_1, i_2,
  \dots, i_k$, 
  \begin{equation}
    P(A_{i_1} \cap A_{i_2} \cap \dots \cap A_{i_k}) = P(A_{i_1})\cdot
    P(A_{i_2})
    \cdot \dots \cdot P(A_{i_k})
  \end{equation}
\end{definition}

\chapter{Discrete Random Variables and Probability Distributions}

\section{Random Variables}

\begin{definition}
  For a given sample space $\mathcal{S}$ of some experiment, a
  \textbf{random variable (rv)} if any rule that associates a number
  with each outcome in $\mathcal{S}$. In mathematical language, a random
  variable is a function whose domain is the sample space and whose
  range is the set of real numbers.
\end{definition}

\begin{definition}
  Any random variable whose only possible values are $0$ and $1$ is
  called a \textbf{Bernoulli random variable}.
\end{definition}

\begin{definition}
  A \textbf{discrete} random variable is an rv whose possible values
  either constitute a finite set or else can be listed in an infinite
  sequence in which there is a first element, a second element, and so
  on. A random variable is \textbf{continuous} if \textit{both} the
  following apply: 

  \begin{enumerate}
    \item Its set of possible values consists either of all numbers in
    a single interval on the number line (possibly infinite in extent,
    e.g., from $-\infty$ to $\infty$) or all numbers in a disjoint union
    of such intervals (e.g., $\left[ 0, 10 \right]\cup \left[ 20, 30
    \right]$).
    \item No possible value of the variable has positive probability,
    that is, \\
    $P(X = c) = 0$ for any possible value $c$.
  \end{enumerate}
\end{definition}

\section{Probability Distributions for Discrete Random Variables}

\begin{definition}
  The \textbf{probability distribution} or \textbf{probability mass
  function} (pmf) of a discrete rv is defined for every number $x$ by
  $p(x) = P(X = x) = P(\text{all} s\in \mathcal{S} : X(s) =
  x$.\footnote{$P(X = x)$ is read ``the probability that the rv $X$
    assumes the value $x$''. For example, $P(X = 2)$ denotes the
  probability that the resulting $X$ value is 2.}
\end{definition}

\begin{definition}
  Suppose $p(x)$ depends on a qunatity that can be assigned any one a
  number of possible values, with each different value determining a
  different probability distribution. Such a quantity is called a
  \textbf{parameter} of the distribution. The collection of all
  probability distributions for different values of the parameter is
  called a \textbf{family} of probability distributions.
\end{definition}

\begin{definition}
  The \textbf{cumulative distribution function} (cdf) $F(x)$ of a
  discrete rv $X$ with pmf $p(x)$ is defined for every number $x$ by
  \begin{equation*}
    F(X) = P(X \leq x) = \sum\limits_{y:y\leq x} p(y)
  \end{equation*}
  For any number $x, F(x)$ is the probability that the observed value of
  $X$ will be at most $x$.
\end{definition}

\begin{prop}
  For any two numbers $a$ and $b$ with $a \leq b$,
  \begin{equation*}
    P(a \leq X \leq b) = F(b) - F(a-)
  \end{equation*}
  where $F(a-)$ represents the maximum of $F(x)$ values to the left of
  $a$. Equivalently, if $a$ is the limit of values of $x$ approaching
  from the left, then $F(a-)$ is the limiting value of $F(x)$. In
  particular, if the only possible values are integers and if $a$ and
  $b$ are integers, then
  \begin{align*}
    P(a \leq X \leq b) &= P(X = a \text{ or } a+1 \text{ or } \dots \text{ or }
    b) \\
    &= F(b) - F(a-1)
  \end{align*}
  Taking $a = b$ yields $P(X = a) = F(a) - F(a-1)$ in this case.
\end{prop}
\end{document}